{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import required libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a directory for storing model weights\n",
    "output_dir=r'C:\\Users\\DELL\\Documents\\ML4SCI\\basic-model-weights'\n",
    "os.makedirs(output_dir,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-03-17T06:21:11.188585Z",
     "iopub.status.busy": "2025-03-17T06:21:11.188129Z",
     "iopub.status.idle": "2025-03-17T06:21:11.215318Z",
     "shell.execute_reply": "2025-03-17T06:21:11.214395Z",
     "shell.execute_reply.started": "2025-03-17T06:21:11.188561Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Load target dataset\n",
    "def load_data(df):\n",
    "    df=pd.read_csv(df)\n",
    "    return df\n",
    "\n",
    "## Map Features with corresponding filenames in target dataset and form a feature data\n",
    "def map_features_to_names(feature_paths,file_names):\n",
    "  features=[]\n",
    "\n",
    "  file_names=list(file_names)\n",
    "\n",
    "  for filename in os.listdir(feature_paths):\n",
    "    file_path=os.path.join(feature_paths,filename)\n",
    "    if os.path.isfile(file_path):\n",
    "      if filename in file_names:\n",
    "        features.append({'Filepath':file_path,'Filename':filename})\n",
    "  return pd.DataFrame(features)\n",
    "\n",
    "## Join feature data with target data\n",
    "def get_raw_train_data(feature_data,df,filename,input,target):\n",
    "    joined_df=pd.merge(feature_data,df,on=filename,how='inner')\n",
    "    train_df=joined_df[[input,target]]\n",
    "    return train_df\n",
    "\n",
    "## Extract content from feature files\n",
    "def preprocess_inputs(input_file):\n",
    "   file=open(input_file,'r')\n",
    "   inp=file.read()\n",
    "   return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130, 35)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Feynman_with_units_path=r'C:\\Users\\DELL\\Documents\\ML4SCI\\FeynmanEquations.csv'\n",
    "data=load_data(Feynman_with_units_path)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filepath</th>\n",
       "      <th>Filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\DELL\\Documents\\ML4SCI\\Feynman_with_un...</td>\n",
       "      <td>I.10.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\DELL\\Documents\\ML4SCI\\Feynman_with_un...</td>\n",
       "      <td>I.11.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\DELL\\Documents\\ML4SCI\\Feynman_with_un...</td>\n",
       "      <td>I.12.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\DELL\\Documents\\ML4SCI\\Feynman_with_un...</td>\n",
       "      <td>I.12.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\DELL\\Documents\\ML4SCI\\Feynman_with_un...</td>\n",
       "      <td>I.12.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Filepath Filename\n",
       "0  C:\\Users\\DELL\\Documents\\ML4SCI\\Feynman_with_un...   I.10.7\n",
       "1  C:\\Users\\DELL\\Documents\\ML4SCI\\Feynman_with_un...  I.11.19\n",
       "2  C:\\Users\\DELL\\Documents\\ML4SCI\\Feynman_with_un...   I.12.1\n",
       "3  C:\\Users\\DELL\\Documents\\ML4SCI\\Feynman_with_un...  I.12.11\n",
       "4  C:\\Users\\DELL\\Documents\\ML4SCI\\Feynman_with_un...   I.12.2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Features_path=r'C:\\Users\\DELL\\Documents\\ML4SCI\\Feynman_with_units\\Feynman_with_units'\n",
    "file_names=data['Filename']\n",
    "data_with_feats=map_features_to_names(Features_path,file_names)\n",
    "data_with_feats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filepath</th>\n",
       "      <th>Formula</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\DELL\\Documents\\ML4SCI\\Feynman_with_un...</td>\n",
       "      <td>m_0/sqrt(1-v**2/c**2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\DELL\\Documents\\ML4SCI\\Feynman_with_un...</td>\n",
       "      <td>x1*y1+x2*y2+x3*y3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\DELL\\Documents\\ML4SCI\\Feynman_with_un...</td>\n",
       "      <td>mu*Nn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\DELL\\Documents\\ML4SCI\\Feynman_with_un...</td>\n",
       "      <td>q*(Ef+B*v*sin(theta))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\DELL\\Documents\\ML4SCI\\Feynman_with_un...</td>\n",
       "      <td>q1*q2*r/(4*pi*epsilon*r**3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Filepath  \\\n",
       "0  C:\\Users\\DELL\\Documents\\ML4SCI\\Feynman_with_un...   \n",
       "1  C:\\Users\\DELL\\Documents\\ML4SCI\\Feynman_with_un...   \n",
       "2  C:\\Users\\DELL\\Documents\\ML4SCI\\Feynman_with_un...   \n",
       "3  C:\\Users\\DELL\\Documents\\ML4SCI\\Feynman_with_un...   \n",
       "4  C:\\Users\\DELL\\Documents\\ML4SCI\\Feynman_with_un...   \n",
       "\n",
       "                       Formula  \n",
       "0        m_0/sqrt(1-v**2/c**2)  \n",
       "1            x1*y1+x2*y2+x3*y3  \n",
       "2                        mu*Nn  \n",
       "3        q*(Ef+B*v*sin(theta))  \n",
       "4  q1*q2*r/(4*pi*epsilon*r**3)  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data=get_raw_train_data(data_with_feats,data,'Filename','Filepath','Formula')\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filepath</th>\n",
       "      <th>Formula</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.6464076172823914 1.5889609877804642 5.755668...</td>\n",
       "      <td>m_0/sqrt(1-v**2/c**2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.6823171277495557 2.849193436404849 2.5054347...</td>\n",
       "      <td>x1*y1+x2*y2+x3*y3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.1136223150728854 4.237022171434605 4.7184424...</td>\n",
       "      <td>mu*Nn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.6988691007665642 1.3752399346386226 2.472610...</td>\n",
       "      <td>q*(Ef+B*v*sin(theta))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.5129800991097357 4.924224713199264 4.0911408...</td>\n",
       "      <td>q1*q2*r/(4*pi*epsilon*r**3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Filepath  \\\n",
       "0  1.6464076172823914 1.5889609877804642 5.755668...   \n",
       "1  1.6823171277495557 2.849193436404849 2.5054347...   \n",
       "2  1.1136223150728854 4.237022171434605 4.7184424...   \n",
       "3  1.6988691007665642 1.3752399346386226 2.472610...   \n",
       "4  3.5129800991097357 4.924224713199264 4.0911408...   \n",
       "\n",
       "                       Formula  \n",
       "0        m_0/sqrt(1-v**2/c**2)  \n",
       "1            x1*y1+x2*y2+x3*y3  \n",
       "2                        mu*Nn  \n",
       "3        q*(Ef+B*v*sin(theta))  \n",
       "4  q1*q2*r/(4*pi*epsilon*r**3)  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['Filepath']=training_data['Filepath'].map(preprocess_inputs)\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.to_csv('training_data.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Tokenizer and Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')## Load BERT Tokenizer\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')## Load BERT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-03-17T07:43:47.009Z",
     "iopub.execute_input": "2025-03-17T06:50:52.252402Z",
     "iopub.status.busy": "2025-03-17T06:50:52.251984Z"
    },
    "id": "lb7EzENLVdcc",
    "outputId": "9806d6d3-aefd-408b-d84e-ffc365a41b3b",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done : 0\n",
      "Done : 1\n",
      "Done : 2\n",
      "Done : 3\n",
      "Done : 4\n",
      "Done : 5\n",
      "Done : 6\n",
      "Done : 7\n",
      "Done : 8\n",
      "Done : 9\n",
      "Done : 10\n",
      "Done : 11\n",
      "Done : 12\n",
      "Done : 13\n",
      "Done : 14\n",
      "Done : 15\n",
      "Done : 16\n",
      "Done : 17\n",
      "Done : 18\n",
      "Done : 19\n",
      "Done : 20\n",
      "Done : 21\n",
      "Done : 22\n",
      "Done : 23\n",
      "Done : 24\n",
      "Done : 25\n",
      "Done : 26\n",
      "Done : 27\n",
      "Done : 28\n",
      "Done : 29\n",
      "Done : 30\n",
      "Done : 31\n",
      "Done : 32\n",
      "Done : 33\n",
      "Done : 34\n",
      "Done : 35\n",
      "Done : 36\n",
      "Done : 37\n",
      "Done : 38\n",
      "Done : 39\n",
      "Done : 40\n",
      "Done : 41\n",
      "Done : 42\n",
      "Done : 43\n",
      "Done : 44\n",
      "Done : 45\n",
      "Done : 46\n",
      "Done : 47\n",
      "Done : 48\n",
      "Done : 49\n",
      "Done : 50\n",
      "Done : 51\n",
      "Done : 52\n",
      "Done : 53\n",
      "Done : 54\n",
      "Done : 55\n",
      "Done : 56\n",
      "Done : 57\n",
      "Done : 58\n",
      "Done : 59\n",
      "Done : 60\n",
      "Done : 61\n",
      "Done : 62\n",
      "Done : 63\n",
      "Done : 64\n",
      "Done : 65\n",
      "Done : 66\n",
      "Done : 67\n",
      "Done : 68\n",
      "Done : 69\n",
      "Done : 70\n",
      "Done : 71\n",
      "Done : 72\n",
      "Done : 73\n",
      "Done : 74\n",
      "Done : 75\n",
      "Done : 76\n",
      "Done : 77\n",
      "Done : 78\n",
      "Done : 79\n",
      "Done : 80\n",
      "Done : 81\n",
      "Done : 82\n",
      "Done : 83\n",
      "Done : 84\n",
      "Done : 85\n",
      "Done : 86\n",
      "Done : 87\n",
      "Done : 88\n",
      "Done : 89\n",
      "Done : 90\n",
      "Done : 91\n",
      "Done : 92\n",
      "Done : 93\n",
      "Done : 94\n",
      "Done : 95\n",
      "Done : 96\n",
      "(97, 2)\n"
     ]
    }
   ],
   "source": [
    "## Tokenization\n",
    "def tokenize(target_data):\n",
    "  target_encodings=tokenizer(\n",
    "      target_data,\n",
    "      max_length=128,\n",
    "      padding='max_length',\n",
    "      truncation=True,\n",
    "      return_tensors='pt'\n",
    "  )\n",
    "  return target_encodings\n",
    "\n",
    "## Apply tokenization and get data containing features and targets\n",
    "feats=[]## Features list\n",
    "targets=[]## Targets list\n",
    "data=pd.DataFrame()## DataFrame for features and targets\n",
    "for i in range(0,len(training_data['Formula'])):\n",
    "     inp_encoded=tokenize(training_data['Filepath'][i])## Tokenize features\n",
    "     out_encoded=tokenize(training_data['Formula'][i])## Tokenize targets\n",
    "     #target=target.flatten()\n",
    "     feats.append(inp_encoded)\n",
    "     targets.append(out_encoded)\n",
    "     print(f'Done : {i}')\n",
    "data['Features']=feats\n",
    "data['Targets']=targets\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting of prepared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:(78, 2), Validation data shape:(9, 2), Testing data shape:(10, 2)\n"
     ]
    }
   ],
   "source": [
    "## Splitting data into training data, validation data and testing data\n",
    "train_data,test_data=train_test_split(data,test_size=0.1,random_state=42)## Data into Training data and Testing data\n",
    "train_data,val_data=train_test_split(train_data,test_size=0.1,random_state=42)## Training data into Training data and Validation data\n",
    "print(f'Training data shape:{train_data.shape}, Validation data shape:{val_data.shape}, Testing data shape:{test_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    }
   ],
   "source": [
    "## Convert features and targets into separate lists for training\n",
    "train_inputs = list(train_data['Features'])## Training data features\n",
    "train_outputs = list(train_data['Targets'])## Training data targets\n",
    "val_inputs = list(val_data['Features'])## Validation data features\n",
    "val_outputs = list(val_data['Targets'])## Validation data targets\n",
    "print(type(train_inputs[0]))\n",
    "print(type(val_inputs[0]))\n",
    "print(type(train_outputs[0]))\n",
    "print(type(val_outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": false,
    "execution": {
     "execution_failed": "2025-03-17T07:43:47.010Z"
    },
    "id": "i4lh72MyyF6-",
    "outputId": "8acab45f-119c-48b7-e775-37fe94b0619a",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0----> Train_loss :4.42942\n",
      "Saved weights at step 1\n",
      "              Validation_loss :3.82955\n",
      "Epoch :1----> Train_loss :3.59224\n",
      "              Validation_loss :3.83394\n",
      "Epoch :2----> Train_loss :3.56773\n",
      "              Validation_loss :3.79925\n",
      "Epoch :3----> Train_loss :3.52066\n",
      "              Validation_loss :3.80022\n",
      "Epoch :4----> Train_loss :3.50960\n",
      "              Validation_loss :3.80976\n",
      "Epoch :5----> Train_loss :3.50734\n",
      "              Validation_loss :3.79178\n",
      "Epoch :6----> Train_loss :3.49161\n",
      "              Validation_loss :3.78172\n",
      "Epoch :7----> Train_loss :3.48461\n",
      "              Validation_loss :3.82335\n",
      "Epoch :8----> Train_loss :3.47976\n",
      "              Validation_loss :3.79102\n",
      "Epoch :9----> Train_loss :3.47293\n",
      "              Validation_loss :3.77926\n",
      "Epoch :10----> Train_loss :3.45455\n",
      "              Validation_loss :3.82262\n",
      "Epoch :11----> Train_loss :3.46036\n",
      "              Validation_loss :3.81900\n",
      "Epoch :12----> Train_loss :3.45217\n",
      "              Validation_loss :3.78189\n",
      "Epoch :13----> Train_loss :3.43077\n",
      "              Validation_loss :3.80501\n",
      "Epoch :14----> Train_loss :3.44720\n",
      "              Validation_loss :3.85714\n",
      "Epoch :15----> Train_loss :3.45450\n",
      "              Validation_loss :3.78438\n",
      "Epoch :16----> Train_loss :3.43478\n",
      "              Validation_loss :3.80093\n",
      "Epoch :17----> Train_loss :3.43224\n",
      "              Validation_loss :3.83103\n",
      "Epoch :18----> Train_loss :3.44766\n",
      "              Validation_loss :3.80722\n",
      "Epoch :19----> Train_loss :3.44143\n",
      "              Validation_loss :3.78613\n",
      "Epoch :20----> Train_loss :3.43174\n",
      "              Validation_loss :3.81681\n",
      "Epoch :21----> Train_loss :3.43238\n",
      "              Validation_loss :3.89171\n",
      "Epoch :22----> Train_loss :3.44698\n",
      "              Validation_loss :3.82602\n",
      "Epoch :23----> Train_loss :3.43836\n",
      "              Validation_loss :3.79724\n",
      "Epoch :24----> Train_loss :3.42240\n",
      "              Validation_loss :3.84111\n",
      "Epoch :25----> Train_loss :3.43750\n",
      "              Validation_loss :3.86464\n",
      "Epoch :26----> Train_loss :3.42769\n",
      "              Validation_loss :3.80311\n",
      "Epoch :27----> Train_loss :3.42843\n",
      "              Validation_loss :3.80133\n",
      "Epoch :28----> Train_loss :3.41934\n",
      "              Validation_loss :3.86906\n",
      "Epoch :29----> Train_loss :3.43456\n",
      "              Validation_loss :3.94018\n",
      "Epoch :30----> Train_loss :3.43670\n",
      "              Validation_loss :3.83039\n",
      "Epoch :31----> Train_loss :3.42559\n",
      "              Validation_loss :3.86882\n",
      "Epoch :32----> Train_loss :3.41977\n",
      "              Validation_loss :3.89755\n",
      "Epoch :33----> Train_loss :3.42211\n",
      "              Validation_loss :3.87935\n",
      "Epoch :34----> Train_loss :3.43001\n",
      "              Validation_loss :3.83536\n",
      "Epoch :35----> Train_loss :3.42158\n",
      "              Validation_loss :3.88725\n",
      "Epoch :36----> Train_loss :3.41602\n",
      "              Validation_loss :3.92247\n",
      "Epoch :37----> Train_loss :3.42727\n",
      "              Validation_loss :3.92937\n",
      "Epoch :38----> Train_loss :3.44408\n",
      "              Validation_loss :3.85025\n",
      "Epoch :39----> Train_loss :3.42767\n",
      "              Validation_loss :3.88349\n",
      "Epoch :40----> Train_loss :3.42327\n",
      "              Validation_loss :3.92014\n",
      "Epoch :41----> Train_loss :3.42274\n",
      "              Validation_loss :3.88799\n",
      "Epoch :42----> Train_loss :3.43061\n",
      "              Validation_loss :3.86572\n",
      "Epoch :43----> Train_loss :3.42186\n",
      "              Validation_loss :3.87919\n",
      "Epoch :44----> Train_loss :3.41481\n",
      "              Validation_loss :3.88387\n",
      "Epoch :45----> Train_loss :3.41309\n",
      "              Validation_loss :3.92890\n",
      "Epoch :46----> Train_loss :3.42804\n",
      "              Validation_loss :3.88309\n",
      "Epoch :47----> Train_loss :3.43487\n",
      "              Validation_loss :3.88945\n",
      "Epoch :48----> Train_loss :3.42397\n",
      "              Validation_loss :3.88585\n",
      "Epoch :49----> Train_loss :3.41908\n",
      "              Validation_loss :3.88457\n",
      "Epoch :50----> Train_loss :3.42092\n",
      "Saved weights at step 51\n",
      "              Validation_loss :3.90051\n",
      "Epoch :51----> Train_loss :3.41939\n",
      "              Validation_loss :3.86595\n",
      "Epoch :52----> Train_loss :3.41324\n",
      "              Validation_loss :3.88753\n",
      "Epoch :53----> Train_loss :3.41989\n",
      "              Validation_loss :3.88137\n",
      "Epoch :54----> Train_loss :3.42094\n",
      "              Validation_loss :3.88861\n",
      "Epoch :55----> Train_loss :3.41749\n",
      "              Validation_loss :3.86746\n",
      "Epoch :56----> Train_loss :3.40834\n",
      "              Validation_loss :3.84089\n",
      "Epoch :57----> Train_loss :3.41029\n",
      "              Validation_loss :3.87378\n",
      "Epoch :58----> Train_loss :3.41835\n",
      "              Validation_loss :3.84728\n",
      "Epoch :59----> Train_loss :3.42167\n",
      "              Validation_loss :3.83391\n",
      "Epoch :60----> Train_loss :3.41450\n",
      "              Validation_loss :3.84553\n",
      "Epoch :61----> Train_loss :3.40595\n",
      "              Validation_loss :3.82861\n",
      "Epoch :62----> Train_loss :3.41920\n",
      "              Validation_loss :3.85908\n",
      "Epoch :63----> Train_loss :3.43541\n",
      "              Validation_loss :3.83927\n",
      "Epoch :64----> Train_loss :3.42325\n",
      "              Validation_loss :3.83412\n",
      "Epoch :65----> Train_loss :3.41442\n",
      "              Validation_loss :3.83673\n",
      "Epoch :66----> Train_loss :3.42315\n",
      "              Validation_loss :3.82177\n",
      "Epoch :67----> Train_loss :3.43678\n",
      "              Validation_loss :3.79751\n",
      "Epoch :68----> Train_loss :3.42805\n",
      "              Validation_loss :3.82612\n",
      "Epoch :69----> Train_loss :3.41518\n",
      "              Validation_loss :3.81388\n",
      "Epoch :70----> Train_loss :3.42621\n",
      "              Validation_loss :3.81004\n",
      "Epoch :71----> Train_loss :3.43670\n",
      "              Validation_loss :3.81505\n",
      "Epoch :72----> Train_loss :3.43646\n",
      "              Validation_loss :3.82857\n",
      "Epoch :73----> Train_loss :3.42059\n",
      "              Validation_loss :3.84116\n",
      "Epoch :74----> Train_loss :3.42457\n",
      "              Validation_loss :3.78981\n",
      "Epoch :75----> Train_loss :3.42999\n",
      "              Validation_loss :3.82770\n",
      "Epoch :76----> Train_loss :3.43343\n",
      "              Validation_loss :3.81938\n",
      "Epoch :77----> Train_loss :3.42170\n",
      "              Validation_loss :3.89113\n",
      "Epoch :78----> Train_loss :3.41632\n",
      "              Validation_loss :3.85836\n",
      "Epoch :79----> Train_loss :3.41994\n",
      "              Validation_loss :3.81343\n",
      "Epoch :80----> Train_loss :3.42903\n",
      "              Validation_loss :3.82994\n",
      "Epoch :81----> Train_loss :3.42474\n",
      "              Validation_loss :3.86063\n",
      "Epoch :82----> Train_loss :3.41376\n",
      "              Validation_loss :3.85834\n",
      "Epoch :83----> Train_loss :3.41937\n",
      "              Validation_loss :3.84329\n",
      "Epoch :84----> Train_loss :3.43203\n",
      "              Validation_loss :3.83346\n",
      "Epoch :85----> Train_loss :3.43447\n",
      "              Validation_loss :3.84489\n",
      "Epoch :86----> Train_loss :3.41370\n",
      "              Validation_loss :3.87713\n",
      "Epoch :87----> Train_loss :3.41705\n",
      "              Validation_loss :3.87987\n",
      "Epoch :88----> Train_loss :3.42777\n",
      "              Validation_loss :3.86157\n",
      "Epoch :89----> Train_loss :3.43541\n",
      "              Validation_loss :3.86405\n",
      "Epoch :90----> Train_loss :3.41779\n",
      "              Validation_loss :3.89246\n",
      "Epoch :91----> Train_loss :3.40792\n",
      "              Validation_loss :3.85449\n",
      "Epoch :92----> Train_loss :3.41068\n",
      "              Validation_loss :3.83417\n",
      "Epoch :93----> Train_loss :3.42820\n",
      "              Validation_loss :3.82513\n",
      "Epoch :94----> Train_loss :3.42879\n",
      "              Validation_loss :3.83981\n",
      "Epoch :95----> Train_loss :3.41264\n",
      "              Validation_loss :3.85585\n",
      "Epoch :96----> Train_loss :3.42398\n",
      "              Validation_loss :3.91626\n",
      "Epoch :97----> Train_loss :3.43778\n",
      "              Validation_loss :3.83401\n",
      "Epoch :98----> Train_loss :3.42271\n",
      "              Validation_loss :3.84764\n",
      "Epoch :99----> Train_loss :3.41186\n",
      "              Validation_loss :3.86230\n"
     ]
    }
   ],
   "source": [
    "class TranformerModel(nn.Module):\n",
    "    def __init__(self,bert_model,vocab_size):\n",
    "        super(TranformerModel,self).__init__()\n",
    "        self.bert=bert_model\n",
    "        self.decoder=nn.Linear(768,vocab_size)\n",
    "        \n",
    "    def forward(self,input_ids,attention_mask):\n",
    "        outputs=self.bert(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        last_hidden_state=outputs.last_hidden_state\n",
    "        logits=self.decoder(last_hidden_state)\n",
    "        return logits\n",
    "    \n",
    "model=TranformerModel(bert_model,tokenizer.vocab_size)\n",
    "optimizer=AdamW(model.parameters(),lr=0.01)\n",
    "criterion=nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "epochs=100\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(0,epochs):\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    for i in range(0,len(train_inputs)):\n",
    "        train_input_ids=train_inputs[i]['input_ids'].to(device)\n",
    "        train_attention_mask = train_inputs[i]['attention_mask'].to(device)\n",
    "        train_output_ids = train_outputs[i]['input_ids'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits=model(train_input_ids,train_attention_mask)\n",
    "        loss=criterion(logits.view(-1,logits.size(-1)),train_output_ids.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss+=loss.item()\n",
    "    \n",
    "    avg_loss=total_loss/len(train_inputs)\n",
    "    print(f'Epoch :{epoch}----> Train_loss :{avg_loss:.5f}')\n",
    "    if epoch%50==0:\n",
    "        weights_path=os.path.join(output_dir,f'weights_step_{epoch+1}.pth')\n",
    "        torch.save(model.state_dict(),weights_path)\n",
    "        print(f'Saved weights at step {epoch+1}')\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss=0\n",
    "    with torch.no_grad():\n",
    "        for i in range(0,len(val_inputs)):\n",
    "            val_input_ids=val_inputs[i]['input_ids'].to(device)\n",
    "            val_attention_mask = val_inputs[i]['attention_mask'].to(device)\n",
    "            val_output_ids = val_outputs[i]['input_ids'].to(device)\n",
    "            \n",
    "            logits=model(val_input_ids,val_attention_mask)\n",
    "            loss=criterion(logits.view(-1,logits.size(-1)),val_output_ids.view(-1))\n",
    "            val_loss+=loss.item()\n",
    "            \n",
    "        avg_val_loss=val_loss/len(val_inputs)\n",
    "        print(f'              Validation_loss :{avg_val_loss:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\DELL\\\\Documents\\\\ML4SCI\\\\basic-bert-tokenizer\\\\tokenizer_config.json',\n",
       " 'C:\\\\Users\\\\DELL\\\\Documents\\\\ML4SCI\\\\basic-bert-tokenizer\\\\special_tokens_map.json',\n",
       " 'C:\\\\Users\\\\DELL\\\\Documents\\\\ML4SCI\\\\basic-bert-tokenizer\\\\vocab.txt',\n",
       " 'C:\\\\Users\\\\DELL\\\\Documents\\\\ML4SCI\\\\basic-bert-tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir=r'C:\\Users\\DELL\\Documents\\ML4SCI\\basic-bert-tokenizer'\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\DELL\\\\Documents\\\\ML4SCI\\\\basic-bert-model.joblib']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "model_dir=r'C:\\Users\\DELL\\Documents\\ML4SCI\\basic-bert-model.joblib'\n",
    "joblib.dump(model,model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_data,'tokenized_train.pt')\n",
    "torch.save(test_data,'tokenized_test.pt')\n",
    "torch.save(val_data,'tokenized_val.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "def predict(model,input):\n",
    "    model.eval()\n",
    "    #input_tokens=tokenizer(\n",
    "        #input,\n",
    "        #max_length=128,\n",
    "        #padding='max_length',\n",
    "        #truncation=True,\n",
    "        #return_tensors='pt'\n",
    "    #).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits=model(input['input_ids'].to(device),input['attention_mask'].to(device))\n",
    "        predicted_tokens=torch.argmax(logits,dim=-1)\n",
    "        return predicted_tokens\n",
    "\n",
    "test_inputs=list(test_data['Features'])\n",
    "test_outputs=list(test_data['Targets'])\n",
    "\n",
    "for i in range(0,len(test_inputs)):\n",
    "    pred=predict(model,test_inputs[i])\n",
    "    print(torch.equal(pred,test_outputs[i]['input_ids']))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "datasetId": 6885392,
     "sourceId": 11052016,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6888231,
     "sourceId": 11056160,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
